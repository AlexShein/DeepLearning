{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from confusion_matrix import plot_confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # early stopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "# rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 333\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_RESULTS_FOLDER = 'csv_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "SEQ_LEN = 50\n",
    "NUCLEOTIDES_COUNT = 4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 48, 100)           1300      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 46, 100)           30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 11, 100)           50100     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 94,686\n",
      "Trainable params: 94,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv1D(100, 3, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model1.add(Conv1D(100, 3, activation='relu'))\n",
    "model1.add(MaxPooling1D(3))\n",
    "model1.add(Conv1D(100, 5, activation='relu',))\n",
    "model1.add(GlobalAveragePooling1D())\n",
    "model1.add(Dropout(0.25))\n",
    "# model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'3 CNN Layers': model1,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(sequence):\n",
    "    seq_array = np.array(list(sequence))\n",
    "\n",
    "    # one hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    # reshape because that's what OneHotEncoder likes\n",
    "    seq_array = seq_array.reshape(len(seq_array), 1)\n",
    "    onehot_encoded_seq = onehot_encoder.fit_transform(seq_array)\n",
    "    return onehot_encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframes(target_pathes, non_target_pathes):\n",
    "\n",
    "    target_sequences = None\n",
    "    \n",
    "    for target_path in target_pathes:\n",
    "        with open(target_path, 'r') as file:\n",
    "            target_sequences = (\n",
    "                target_sequences and np.concatenate(\n",
    "                target_sequences,\n",
    "                np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "                ) or\n",
    "                np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "            )\n",
    "    \n",
    "    non_target_sequences = None\n",
    "    for non_target_path in non_target_pathes:\n",
    "        with open(non_target_path, 'r') as file:\n",
    "            non_target_sequences = (\n",
    "                non_target_sequences and np.concatenate(\n",
    "                non_target_sequences,\n",
    "                np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "                ) or \n",
    "                np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "            )\n",
    "                                    \n",
    "    return (target_sequences, non_target_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datasets(target_sequences, non_target_sequences):\n",
    "    \n",
    "    if target_sequences.shape[0] > non_target_sequences.shape[0]:\n",
    "        target_sequences_n = np.random.choice(\n",
    "            target_sequences,\n",
    "            non_target_sequences.shape[0],\n",
    "        )\n",
    "        non_target_sequences_n = non_target_sequences\n",
    "    else:\n",
    "        target_sequences_n = target_sequences\n",
    "        non_target_sequences_n = np.random.choice(\n",
    "            non_target_sequences,\n",
    "            target_sequences.shape[0],\n",
    "        )\n",
    "\n",
    "    X = np.concatenate((target_sequences_n, non_target_sequences_n))\n",
    "    Y = pd.Series(np.append(\n",
    "        np.full(target_sequences_n.shape[0], 1),\n",
    "        np.full(non_target_sequences_n.shape[0], 0))\n",
    "    )\n",
    "    \n",
    "    X = np.array([one_hot_dna(line) for line in X])\n",
    "    Y = keras.utils.to_categorical(Y)\n",
    "    \n",
    "    return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(X, Y, model):\n",
    "    # ROC vars\n",
    "    tprs = []\n",
    "    aucs, acc, rec, prec = [], [], [], []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    i = 0\n",
    "    # Precision-recall vars\n",
    "    precisions = []\n",
    "    best_precision = {\"precision_score\": 0.0, \"precision\": None, \"recall\": None}\n",
    "\n",
    "    folded_data = KFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "    for k, (train, test) in enumerate(folded_data.split(X, Y)):\n",
    "        Y_test_flat = np.array(list(map(lambda x: x[1] == 1 and 1 or 0, Y[test])))\n",
    "        model.fit(\n",
    "            X[train],\n",
    "            Y[train],\n",
    "            # Keras special args\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            validation_split=0.1,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=5)],\n",
    "            )\n",
    "        probas_ = model.predict(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test_flat, probas_[:, 1])\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        Y_pred = model.predict_classes(X[test])\n",
    "\n",
    "        acc.append(accuracy_score(Y_test_flat, Y_pred))\n",
    "        prec.append(precision_score(Y_test_flat, Y_pred))\n",
    "        rec.append(recall_score(Y_test_flat, Y_pred))\n",
    "        # Compute precision, recall\n",
    "        precision, recall, _ = precision_recall_curve(Y_test_flat, probas_[:, 1])\n",
    "        average_precision = average_precision_score(Y_test_flat, probas_[:, 1])\n",
    "        if average_precision > best_precision[\"precision_score\"]:\n",
    "            best_precision[\"precision\"] = precision\n",
    "            best_precision[\"recall\"] = recall\n",
    "            best_precision[\"precision_score\"] = average_precision\n",
    "        precisions.append(average_precision)\n",
    "        \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "\n",
    "        \n",
    "    return mean_fpr, mean_tpr, best_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(datasets, model, model_name):\n",
    "    '''\n",
    "    datasets: a dict like {\n",
    "    'L1': 'data_link/L1/data/all_50_last.txt',\n",
    "    'p_pseudo': 'pseudogenes_50_last.txt',\n",
    "    }\n",
    "    model: keras model\n",
    "    model_name: str\n",
    "    '''\n",
    "\n",
    "    CLASS_1, CLASS_2 = datasets.keys()\n",
    "    target_pathes, non_target_pathes = datasets.values()\n",
    "    # Read data\n",
    "    target_sequences, non_target_sequences = read_dataframes(target_pathes, non_target_pathes)\n",
    "    X, Y = normalize_datasets(target_sequences, non_target_sequences)\n",
    "\n",
    "    # Use 5 fold to evaluate model\n",
    "    mean_fpr, mean_tpr, best_precision = compute_metrics(X, Y, model)\n",
    "\n",
    "    # Create file names\n",
    "    CSV_FILE_SUBNAME_OBJECTS = f\"{CLASS_1}_vs_{CLASS_2}\" # \"True_vs_False\"\n",
    "    CSV_FILE_SUBNAME = f\"{CSV_FILE_SUBNAME_OBJECTS}_{model_name}\"\n",
    "    \n",
    "    # Write results to csv\n",
    "    # ROC\n",
    "    pd.DataFrame({\n",
    "        \"fpr\": mean_fpr,\n",
    "        \"tpr\": mean_tpr\n",
    "    }).to_csv(\n",
    "        f\"{CSV_RESULTS_FOLDER}/ROC__{CSV_FILE_SUBNAME}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    # Precision-recall\n",
    "    pd.DataFrame({\n",
    "        \"precision\": best_precision[\"precision\"],\n",
    "        \"recall\": best_precision[\"recall\"]\n",
    "    }).to_csv(\n",
    "        f\"{CSV_RESULTS_FOLDER}/Precision-Recall__{CSV_FILE_SUBNAME}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    #     pd.DataFrame(fi).to_csv(\"Feature_importance__{0}.csv\".format(CSV_FILE_SUBNAME), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'p_pseudo': ['pseudogenes_50_last.txt',],\n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9535 samples, validate on 1060 samples\n",
      "Epoch 1/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 1.6378e-04 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 9.3262e-04 - acc: 0.9998 - val_loss: 1.3432e-05 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 4.0627e-04 - acc: 0.9999 - val_loss: 7.1182e-05 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 6.0850e-04 - acc: 0.9999 - val_loss: 2.6272e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 6.1704e-05 - acc: 1.0000 - val_loss: 8.9910e-05 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 5.7735e-05 - acc: 1.0000 - val_loss: 1.5722e-04 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "9535/9535 [==============================] - 1s 139us/sample - loss: 3.7882e-04 - acc: 0.9999 - val_loss: 4.5315e-05 - val_acc: 1.0000\n",
      "Train on 9535 samples, validate on 1060 samples\n",
      "Epoch 1/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.0079 - val_acc: 0.9981\n",
      "Epoch 2/10\n",
      "9535/9535 [==============================] - 1s 136us/sample - loss: 6.2726e-04 - acc: 0.9997 - val_loss: 1.3175e-04 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "9535/9535 [==============================] - 1s 136us/sample - loss: 0.0013 - acc: 0.9994 - val_loss: 5.8583e-04 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 0.0028 - acc: 0.9992 - val_loss: 1.7660e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 3.9756e-04 - acc: 0.9999 - val_loss: 3.1492e-04 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 7.8449e-04 - acc: 0.9997 - val_loss: 2.3098e-05 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0019 - val_acc: 0.9991\n",
      "Epoch 9/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 6.3232e-04 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 5.4283e-05 - acc: 1.0000 - val_loss: 7.9006e-05 - val_acc: 1.0000\n",
      "Train on 9535 samples, validate on 1060 samples\n",
      "Epoch 1/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 1.0194e-05 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 5.2098e-06 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 8.8333e-04 - acc: 0.9998 - val_loss: 6.2663e-05 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "9535/9535 [==============================] - 1s 136us/sample - loss: 9.0149e-04 - acc: 0.9998 - val_loss: 7.1999e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 1.3563e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 6/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 2.0932e-04 - acc: 0.9999 - val_loss: 4.3602e-04 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 7.0345e-04 - acc: 0.9998 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Train on 9535 samples, validate on 1060 samples\n",
      "Epoch 1/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 1.2966e-04 - acc: 1.0000 - val_loss: 4.5488e-04 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "9535/9535 [==============================] - 1s 136us/sample - loss: 1.6432e-05 - acc: 1.0000 - val_loss: 3.3189e-04 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 3.0625e-05 - acc: 1.0000 - val_loss: 7.6266e-04 - val_acc: 0.9991\n",
      "Epoch 4/10\n",
      "9535/9535 [==============================] - 1s 136us/sample - loss: 1.8161e-05 - acc: 1.0000 - val_loss: 4.6903e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 1.2140e-05 - acc: 1.0000 - val_loss: 3.6088e-04 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "9535/9535 [==============================] - 1s 136us/sample - loss: 1.7130e-05 - acc: 1.0000 - val_loss: 9.6788e-05 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "9535/9535 [==============================] - 1s 138us/sample - loss: 1.6197e-05 - acc: 1.0000 - val_loss: 3.9039e-04 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 5.2501e-06 - acc: 1.0000 - val_loss: 2.9989e-04 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 3.3082e-05 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9991\n",
      "Epoch 10/10\n",
      "9535/9535 [==============================] - 1s 137us/sample - loss: 2.2162e-04 - acc: 0.9999 - val_loss: 9.8929e-04 - val_acc: 0.9991\n",
      "Train on 9536 samples, validate on 1060 samples\n",
      "Epoch 1/10\n",
      "9536/9536 [==============================] - 1s 137us/sample - loss: 1.8958e-05 - acc: 1.0000 - val_loss: 2.0841e-04 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "9536/9536 [==============================] - 1s 138us/sample - loss: 6.7842e-06 - acc: 1.0000 - val_loss: 1.0155e-04 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "9536/9536 [==============================] - 1s 136us/sample - loss: 8.3418e-06 - acc: 1.0000 - val_loss: 1.3049e-04 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "9536/9536 [==============================] - 1s 138us/sample - loss: 8.8351e-06 - acc: 1.0000 - val_loss: 1.5773e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "9536/9536 [==============================] - 1s 136us/sample - loss: 7.1253e-05 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9991\n",
      "Epoch 6/10\n",
      "9536/9536 [==============================] - 1s 136us/sample - loss: 1.1351e-05 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9991\n",
      "Epoch 7/10\n",
      "9536/9536 [==============================] - 1s 136us/sample - loss: 2.6984e-06 - acc: 1.0000 - val_loss: 7.3552e-04 - val_acc: 0.9991\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    for experiment_dataset in experiments:\n",
    "        run_experiment(experiment_dataset, model, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
