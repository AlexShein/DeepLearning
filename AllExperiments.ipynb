{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from confusion_matrix import plot_confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # early stopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "# rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 333\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_RESULTS_FOLDER = 'csv_results'\n",
    "EarlyStoppingCallback = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "SEQ_LEN = 50\n",
    "NUCLEOTIDES_COUNT = 4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 42, 50)            1850      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 1,952\n",
      "Trainable params: 1,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(Conv1D(50, 9, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model0.add(GlobalAveragePooling1D())\n",
    "model0.add(Dropout(0.3))\n",
    "model0.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model0.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexshein/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/alexshein/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 42, 50)            1850      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 5,244\n",
      "Trainable params: 5,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv1D(50, 9, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model1.add(GlobalAveragePooling1D())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 50, 50)            1850      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 14, 50)            7550      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 12,794\n",
      "Trainable params: 12,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv1D(50, 9, activation='relu', padding='same', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model2.add(MaxPooling1D(3))\n",
    "model2.add(Conv1D(50, 3, activation='relu'))\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model2.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 42, 50)            1850      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 14, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 12, 50)            7550      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 2, 50)             7550      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 20,344\n",
      "Trainable params: 20,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv1D(50, 9, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(50, 3, activation='relu'))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(50, 3, activation='relu',))\n",
    "model3.add(GlobalAveragePooling1D())\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model3.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    '3_CNN_Layers': model3,\n",
    "    '2_CNN_Layers': model2,\n",
    "    '1_CNN_Layer': model1,\n",
    "    '1_CNN_Layer_Simple': model1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(sequence):\n",
    "    seq_array = np.array(list(sequence))\n",
    "\n",
    "    # one hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    # reshape because that's what OneHotEncoder likes\n",
    "    seq_array = seq_array.reshape(len(seq_array), 1)\n",
    "    onehot_encoded_seq = onehot_encoder.fit_transform(seq_array)\n",
    "    return onehot_encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframes(target_pathes, non_target_pathes):\n",
    "\n",
    "    target_sequences = None\n",
    "    \n",
    "    for target_path in target_pathes:\n",
    "        with open(target_path, 'r') as file:\n",
    "            read_sequences = np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "            if target_sequences is not None:\n",
    "                target_sequences = np.concatenate([\n",
    "                        target_sequences,\n",
    "                        read_sequences\n",
    "                    ])\n",
    "            else:\n",
    "                target_sequences = read_sequences\n",
    "            \n",
    "    non_target_sequences = None\n",
    "    for non_target_path in non_target_pathes:\n",
    "        with open(non_target_path, 'r') as file:\n",
    "            read_sequences = np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "            if non_target_sequences is not None:\n",
    "                non_target_sequences = np.concatenate([\n",
    "                        non_target_sequences,\n",
    "                        read_sequences\n",
    "                    ]) \n",
    "            else:\n",
    "                non_target_sequences = read_sequences\n",
    "                                    \n",
    "    return (target_sequences, non_target_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datasets(target_sequences, non_target_sequences):\n",
    "    \n",
    "    if target_sequences.shape[0] > non_target_sequences.shape[0]:\n",
    "        target_sequences_n = np.random.choice(\n",
    "            target_sequences,\n",
    "            non_target_sequences.shape[0],\n",
    "        )\n",
    "        non_target_sequences_n = non_target_sequences\n",
    "    else:\n",
    "        target_sequences_n = target_sequences\n",
    "        non_target_sequences_n = np.random.choice(\n",
    "            non_target_sequences,\n",
    "            target_sequences.shape[0],\n",
    "        )\n",
    "\n",
    "    X = np.concatenate((target_sequences_n, non_target_sequences_n))\n",
    "    Y = pd.Series(np.append(\n",
    "        np.full(target_sequences_n.shape[0], 1),\n",
    "        np.full(non_target_sequences_n.shape[0], 0))\n",
    "    )\n",
    "    \n",
    "    X = np.array([one_hot_dna(line) for line in X])\n",
    "    Y = keras.utils.to_categorical(Y)\n",
    "    \n",
    "    return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(X, Y, model):\n",
    "    # ROC vars\n",
    "    tprs = []\n",
    "    aucs, acc, rec, prec = [], [], [], []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    i = 0\n",
    "    # Precision-recall vars\n",
    "    precisions = []\n",
    "    best_precision = {\"precision_score\": 0.0, \"precision\": None, \"recall\": None}\n",
    "\n",
    "    folded_data = KFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "    for k, (train, test) in enumerate(folded_data.split(X, Y)):\n",
    "        Y_test_flat = np.array(list(map(lambda x: x[1] == 1 and 1 or 0, Y[test])))\n",
    "        model.fit(\n",
    "            X[train],\n",
    "            Y[train],\n",
    "            # Keras special args\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            validation_split=0.1,\n",
    "            callbacks=[EarlyStoppingCallback],\n",
    "            )\n",
    "        probas_ = model.predict(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test_flat, probas_[:, 1])\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        Y_pred = model.predict_classes(X[test])\n",
    "\n",
    "        acc.append(accuracy_score(Y_test_flat, Y_pred))\n",
    "        prec.append(precision_score(Y_test_flat, Y_pred))\n",
    "        rec.append(recall_score(Y_test_flat, Y_pred))\n",
    "        # Compute precision, recall\n",
    "        precision, recall, _ = precision_recall_curve(Y_test_flat, probas_[:, 1])\n",
    "        average_precision = average_precision_score(Y_test_flat, probas_[:, 1])\n",
    "        if average_precision > best_precision[\"precision_score\"]:\n",
    "            best_precision[\"precision\"] = precision\n",
    "            best_precision[\"recall\"] = recall\n",
    "            best_precision[\"precision_score\"] = average_precision\n",
    "        precisions.append(average_precision)\n",
    "        \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "\n",
    "        \n",
    "    return mean_fpr, mean_tpr, best_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(datasets, model, model_name, write_results=True):\n",
    "    '''\n",
    "    datasets: a dict like {\n",
    "    'L1': 'data_link/L1/data/all_50_last.txt',\n",
    "    'p_pseudo': 'pseudogenes_50_last.txt',\n",
    "    }\n",
    "    model: keras model\n",
    "    model_name: str\n",
    "    '''\n",
    "    # Restart session to ensure that model is clean\n",
    "    #     keras.backend.clear_session()\n",
    "\n",
    "    CLASS_1, CLASS_2 = datasets.keys()\n",
    "    target_pathes, non_target_pathes = datasets.values()\n",
    "    # Read data\n",
    "    target_sequences, non_target_sequences = read_dataframes(target_pathes, non_target_pathes)\n",
    "    X, Y = normalize_datasets(target_sequences, non_target_sequences)\n",
    "\n",
    "    # Use 5 fold to evaluate model\n",
    "    mean_fpr, mean_tpr, best_precision = compute_metrics(X, Y, model)\n",
    "\n",
    "    if write_results:\n",
    "        # Create file names\n",
    "        CSV_FILE_SUBNAME_OBJECTS = f\"{CLASS_1}_vs_{CLASS_2}\" # \"True_vs_False\"\n",
    "        CSV_FILE_SUBNAME = f\"{CSV_FILE_SUBNAME_OBJECTS}__{model_name}\"\n",
    "\n",
    "        # Write results to csv\n",
    "        # ROC\n",
    "        pd.DataFrame({\n",
    "            \"fpr\": mean_fpr,\n",
    "            \"tpr\": mean_tpr\n",
    "        }).to_csv(\n",
    "            f\"{CSV_RESULTS_FOLDER}/ROC__{CSV_FILE_SUBNAME}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "        # Precision-recall\n",
    "        pd.DataFrame({\n",
    "            \"precision\": best_precision[\"precision\"],\n",
    "            \"recall\": best_precision[\"recall\"]\n",
    "        }).to_csv(\n",
    "            f\"{CSV_RESULTS_FOLDER}/Precision-Recall__{CSV_FILE_SUBNAME}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "        #     pd.DataFrame(fi).to_csv(\"Feature_importance__{0}.csv\".format(CSV_FILE_SUBNAME), index=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'p_pseudo': ['pseudogenes_50_last.txt',],\n",
    "    },\n",
    "    {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'mRNA': ['KnownGene_50_last.txt',],\n",
    "    },\n",
    "    {\n",
    "    'L1_p_pseudo': ['data_link/L1/data/all_50_last.txt', 'pseudogenes_50_last.txt',],\n",
    "    'shuffled': ['data_link/L1/data/all_shuffled_50_last.txt', 'pseudogenes_50_last_shuffled.txt',],\n",
    "    },\n",
    "    {\n",
    "    'L1_mRNA': ['data_link/L1/data/all_50_last.txt', 'KnownGene_50_last.txt',],\n",
    "    'shuffled': ['data_link/L1/data/all_shuffled_50_last.txt', 'KnownGene_50_last_shuffled.txt',],\n",
    "    },\n",
    "    {\n",
    "    'mRNA': ['KnownGene_50_last.txt',],\n",
    "    'shuffled': ['KnownGene_50_last_shuffled.txt',],\n",
    "    },\n",
    "    {\n",
    "    'p_pseudo': ['pseudogenes_50_last.txt',],\n",
    "    'shuffled': ['pseudogenes_50_last_shuffled.txt',],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67745/67745 [==============================] - 2s 26us/sample - loss: 0.4440 - acc: 0.7856 - val_loss: 0.5207 - val_acc: 0.7582\n",
      "Epoch 5/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4432 - acc: 0.7871 - val_loss: 0.5429 - val_acc: 0.7306\n",
      "Epoch 6/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4424 - acc: 0.7873 - val_loss: 0.4704 - val_acc: 0.7965\n",
      "Epoch 7/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4418 - acc: 0.7890 - val_loss: 0.5270 - val_acc: 0.7426\n",
      "Epoch 8/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4403 - acc: 0.7897 - val_loss: 0.4600 - val_acc: 0.8017\n",
      "Train on 67745 samples, validate on 7528 samples\n",
      "Epoch 1/10\n",
      "67745/67745 [==============================] - 2s 26us/sample - loss: 0.4446 - acc: 0.7849 - val_loss: 0.4667 - val_acc: 0.7790\n",
      "Epoch 2/10\n",
      "67745/67745 [==============================] - 2s 26us/sample - loss: 0.4430 - acc: 0.7850 - val_loss: 0.5082 - val_acc: 0.7537\n",
      "Epoch 3/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4406 - acc: 0.7870 - val_loss: 0.4951 - val_acc: 0.7659\n",
      "Epoch 4/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4394 - acc: 0.7872 - val_loss: 0.4209 - val_acc: 0.8361\n",
      "Epoch 5/10\n",
      "67745/67745 [==============================] - 2s 26us/sample - loss: 0.4404 - acc: 0.7867 - val_loss: 0.5015 - val_acc: 0.7549\n",
      "Epoch 6/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4399 - acc: 0.7869 - val_loss: 0.5028 - val_acc: 0.7578\n",
      "Epoch 7/10\n",
      "67745/67745 [==============================] - 2s 26us/sample - loss: 0.4374 - acc: 0.7889 - val_loss: 0.4687 - val_acc: 0.7974\n",
      "Epoch 8/10\n",
      "67745/67745 [==============================] - 2s 26us/sample - loss: 0.4373 - acc: 0.7898 - val_loss: 0.5269 - val_acc: 0.7427\n",
      "Epoch 9/10\n",
      "67745/67745 [==============================] - 2s 25us/sample - loss: 0.4368 - acc: 0.7896 - val_loss: 0.4851 - val_acc: 0.7762\n",
      "Train on 67746 samples, validate on 7528 samples\n",
      "Epoch 1/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4392 - acc: 0.7881 - val_loss: 0.4158 - val_acc: 0.8204\n",
      "Epoch 2/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4383 - acc: 0.7889 - val_loss: 0.5338 - val_acc: 0.7457\n",
      "Epoch 3/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4376 - acc: 0.7888 - val_loss: 0.5289 - val_acc: 0.7584\n",
      "Epoch 4/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4371 - acc: 0.7905 - val_loss: 0.5223 - val_acc: 0.7476\n",
      "Epoch 5/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4359 - acc: 0.7891 - val_loss: 0.3896 - val_acc: 0.8478\n",
      "Epoch 6/10\n",
      "67746/67746 [==============================] - 2s 25us/sample - loss: 0.4360 - acc: 0.7909 - val_loss: 0.4557 - val_acc: 0.8034\n",
      "Epoch 7/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4345 - acc: 0.7924 - val_loss: 0.4375 - val_acc: 0.8110\n",
      "Epoch 8/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4333 - acc: 0.7921 - val_loss: 0.4679 - val_acc: 0.7925\n",
      "Epoch 9/10\n",
      "67746/67746 [==============================] - 2s 25us/sample - loss: 0.4327 - acc: 0.7931 - val_loss: 0.6082 - val_acc: 0.6934\n",
      "Epoch 10/10\n",
      "67746/67746 [==============================] - 2s 25us/sample - loss: 0.4331 - acc: 0.7902 - val_loss: 0.4913 - val_acc: 0.7653\n",
      "Train on 67746 samples, validate on 7528 samples\n",
      "Epoch 1/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4372 - acc: 0.7906 - val_loss: 0.3910 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4354 - acc: 0.7891 - val_loss: 0.4645 - val_acc: 0.7985\n",
      "Epoch 3/10\n",
      "67746/67746 [==============================] - 2s 25us/sample - loss: 0.4340 - acc: 0.7907 - val_loss: 0.4331 - val_acc: 0.8070\n",
      "Epoch 4/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4343 - acc: 0.7912 - val_loss: 0.4244 - val_acc: 0.8187\n",
      "Epoch 5/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4346 - acc: 0.7913 - val_loss: 0.5188 - val_acc: 0.7564\n",
      "Epoch 6/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4338 - acc: 0.7922 - val_loss: 0.4760 - val_acc: 0.7848\n",
      "Train on 67746 samples, validate on 7528 samples\n",
      "Epoch 1/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4346 - acc: 0.7896 - val_loss: 0.5120 - val_acc: 0.7468\n",
      "Epoch 2/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4346 - acc: 0.7901 - val_loss: 0.5097 - val_acc: 0.7626\n",
      "Epoch 3/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4341 - acc: 0.7918 - val_loss: 0.5151 - val_acc: 0.7515\n",
      "Epoch 4/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4337 - acc: 0.7922 - val_loss: 0.5088 - val_acc: 0.7675\n",
      "Epoch 5/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4340 - acc: 0.7917 - val_loss: 0.5658 - val_acc: 0.7167\n",
      "Epoch 6/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4341 - acc: 0.7907 - val_loss: 0.5680 - val_acc: 0.7172\n",
      "Epoch 7/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4347 - acc: 0.7916 - val_loss: 0.4436 - val_acc: 0.8135\n",
      "Epoch 8/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4321 - acc: 0.7918 - val_loss: 0.5102 - val_acc: 0.7480\n",
      "Epoch 9/10\n",
      "67746/67746 [==============================] - 2s 26us/sample - loss: 0.4312 - acc: 0.7920 - val_loss: 0.5054 - val_acc: 0.7523\n",
      "Epoch 10/10\n",
      "67746/67746 [==============================] - 2s 25us/sample - loss: 0.4317 - acc: 0.7906 - val_loss: 0.5717 - val_acc: 0.7140\n",
      "Train on 14357 samples, validate on 1596 samples\n",
      "Epoch 1/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.6423 - acc: 0.6370 - val_loss: 0.7192 - val_acc: 0.6178\n",
      "Epoch 2/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.6221 - acc: 0.6570 - val_loss: 0.6345 - val_acc: 0.7325\n",
      "Epoch 3/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.6066 - acc: 0.6710 - val_loss: 0.6318 - val_acc: 0.7011\n",
      "Epoch 4/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.5968 - acc: 0.6783 - val_loss: 0.7795 - val_acc: 0.5520\n",
      "Epoch 5/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5869 - acc: 0.6846 - val_loss: 0.5269 - val_acc: 0.8039\n",
      "Epoch 6/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5814 - acc: 0.6896 - val_loss: 0.6717 - val_acc: 0.6554\n",
      "Epoch 7/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.5761 - acc: 0.6937 - val_loss: 0.7992 - val_acc: 0.5420\n",
      "Epoch 8/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5685 - acc: 0.6978 - val_loss: 0.6858 - val_acc: 0.6604\n",
      "Epoch 9/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.5651 - acc: 0.7046 - val_loss: 0.6861 - val_acc: 0.6529\n",
      "Epoch 10/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5602 - acc: 0.7072 - val_loss: 0.6056 - val_acc: 0.7262\n",
      "Train on 14357 samples, validate on 1596 samples\n",
      "Epoch 1/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.5596 - acc: 0.7073 - val_loss: 0.6455 - val_acc: 0.6936\n",
      "Epoch 2/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5552 - acc: 0.7111 - val_loss: 0.7344 - val_acc: 0.6253\n",
      "Epoch 3/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5524 - acc: 0.7157 - val_loss: 0.6919 - val_acc: 0.6635\n",
      "Epoch 4/10\n",
      "14357/14357 [==============================] - 0s 27us/sample - loss: 0.5476 - acc: 0.7151 - val_loss: 0.6285 - val_acc: 0.7130\n",
      "Epoch 5/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5464 - acc: 0.7207 - val_loss: 0.7512 - val_acc: 0.6222\n",
      "Epoch 6/10\n",
      "14357/14357 [==============================] - 0s 25us/sample - loss: 0.5448 - acc: 0.7174 - val_loss: 0.6048 - val_acc: 0.7531\n",
      "Epoch 7/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5405 - acc: 0.7213 - val_loss: 0.5959 - val_acc: 0.7393\n",
      "Epoch 8/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5366 - acc: 0.7229 - val_loss: 0.6514 - val_acc: 0.7080\n",
      "Epoch 9/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5353 - acc: 0.7256 - val_loss: 0.5890 - val_acc: 0.7544\n",
      "Epoch 10/10\n",
      "14357/14357 [==============================] - 0s 26us/sample - loss: 0.5361 - acc: 0.7245 - val_loss: 0.5928 - val_acc: 0.7362\n",
      "Train on 14358 samples, validate on 1596 samples\n",
      "Epoch 1/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5418 - acc: 0.7176 - val_loss: 0.6208 - val_acc: 0.7249\n",
      "Epoch 2/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5371 - acc: 0.7200 - val_loss: 0.6656 - val_acc: 0.7043\n",
      "Epoch 3/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5344 - acc: 0.7234 - val_loss: 0.5221 - val_acc: 0.8026\n",
      "Epoch 4/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5308 - acc: 0.7245 - val_loss: 0.5563 - val_acc: 0.7581\n",
      "Epoch 5/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5285 - acc: 0.7288 - val_loss: 0.5907 - val_acc: 0.7400\n",
      "Epoch 6/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5308 - acc: 0.7255 - val_loss: 0.7018 - val_acc: 0.6748\n",
      "Epoch 7/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5279 - acc: 0.7277 - val_loss: 0.5712 - val_acc: 0.7688\n",
      "Epoch 8/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5240 - acc: 0.7309 - val_loss: 0.6810 - val_acc: 0.6823\n",
      "Train on 14358 samples, validate on 1596 samples\n",
      "Epoch 1/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5283 - acc: 0.7302 - val_loss: 0.6799 - val_acc: 0.6798\n",
      "Epoch 2/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5286 - acc: 0.7264 - val_loss: 0.6065 - val_acc: 0.7419\n",
      "Epoch 3/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5254 - acc: 0.7292 - val_loss: 0.6287 - val_acc: 0.7237\n",
      "Epoch 4/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5237 - acc: 0.7313 - val_loss: 0.6324 - val_acc: 0.7243\n",
      "Epoch 5/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5212 - acc: 0.7335 - val_loss: 0.5871 - val_acc: 0.7650\n",
      "Epoch 6/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5209 - acc: 0.7316 - val_loss: 0.6681 - val_acc: 0.7061\n",
      "Epoch 7/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5174 - acc: 0.7369 - val_loss: 0.7010 - val_acc: 0.6711\n",
      "Epoch 8/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5154 - acc: 0.7388 - val_loss: 0.6227 - val_acc: 0.7469\n",
      "Epoch 9/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5146 - acc: 0.7365 - val_loss: 0.6095 - val_acc: 0.7393\n",
      "Epoch 10/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5097 - acc: 0.7393 - val_loss: 0.6813 - val_acc: 0.6855\n",
      "Train on 14358 samples, validate on 1596 samples\n",
      "Epoch 1/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5202 - acc: 0.7337 - val_loss: 0.6613 - val_acc: 0.7168\n",
      "Epoch 2/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5162 - acc: 0.7346 - val_loss: 0.7359 - val_acc: 0.6573\n",
      "Epoch 3/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5167 - acc: 0.7349 - val_loss: 0.6876 - val_acc: 0.7036\n",
      "Epoch 4/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5124 - acc: 0.7363 - val_loss: 0.6855 - val_acc: 0.6917\n",
      "Epoch 5/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5126 - acc: 0.7380 - val_loss: 0.6317 - val_acc: 0.7519\n",
      "Epoch 6/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5110 - acc: 0.7403 - val_loss: 0.8271 - val_acc: 0.6096\n",
      "Epoch 7/10\n",
      "14358/14358 [==============================] - 0s 25us/sample - loss: 0.5095 - acc: 0.7404 - val_loss: 0.5867 - val_acc: 0.7707\n",
      "Epoch 8/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5062 - acc: 0.7401 - val_loss: 0.5440 - val_acc: 0.8020\n",
      "Epoch 9/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5063 - acc: 0.7412 - val_loss: 0.6035 - val_acc: 0.7625\n",
      "Epoch 10/10\n",
      "14358/14358 [==============================] - 0s 26us/sample - loss: 0.5070 - acc: 0.7433 - val_loss: 0.6644 - val_acc: 0.7243\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    for experiment_dataset in experiments:\n",
    "        run_experiment(experiment_dataset, model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
