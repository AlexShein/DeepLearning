{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from confusion_matrix import plot_confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # early stopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "# rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 333\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_RESULTS_FOLDER = 'csv_results'\n",
    "EarlyStoppingCallback = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "SEQ_LEN = 50\n",
    "NUCLEOTIDES_COUNT = 4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexshein/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/alexshein/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 42, 50)            1850      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 1,952\n",
      "Trainable params: 1,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(Conv1D(50, 9, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model0.add(GlobalAveragePooling1D())\n",
    "model0.add(Dropout(0.5))\n",
    "model0.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model0.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 42, 50)            1850      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 5,244\n",
      "Trainable params: 5,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv1D(50, 9, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model1.add(GlobalAveragePooling1D())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 50, 50)            1850      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 14, 50)            7550      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 12,794\n",
      "Trainable params: 12,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv1D(50, 9, activation='relu', padding='same', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model2.add(MaxPooling1D(3))\n",
    "model2.add(Conv1D(50, 3, activation='relu'))\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model2.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 42, 50)            1850      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 14, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 12, 50)            7550      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 2, 50)             7550      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 20,344\n",
      "Trainable params: 20,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv1D(50, 9, activation='relu', input_shape=(SEQ_LEN, NUCLEOTIDES_COUNT)))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(50, 3, activation='relu'))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(50, 3, activation='relu',))\n",
    "model3.add(GlobalAveragePooling1D())\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model3.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    '3_CNN_Layers': model3,\n",
    "    '2_CNN_Layers': model2,\n",
    "    '1_CNN_Layer': model1,\n",
    "    '1_CNN_Layer_Simple': model0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_dna(sequence):\n",
    "    seq_array = np.array(list(sequence))\n",
    "\n",
    "    # one hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    # reshape because that's what OneHotEncoder likes\n",
    "    seq_array = seq_array.reshape(len(seq_array), 1)\n",
    "    onehot_encoded_seq = onehot_encoder.fit_transform(seq_array)\n",
    "    return onehot_encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframes(target_pathes, non_target_pathes):\n",
    "\n",
    "    target_sequences = None\n",
    "    \n",
    "    for target_path in target_pathes:\n",
    "        with open(target_path, 'r') as file:\n",
    "            read_sequences = np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "            if target_sequences is not None:\n",
    "                target_sequences = np.concatenate([\n",
    "                        target_sequences,\n",
    "                        read_sequences\n",
    "                    ])\n",
    "            else:\n",
    "                target_sequences = read_sequences\n",
    "            \n",
    "    non_target_sequences = None\n",
    "    for non_target_path in non_target_pathes:\n",
    "        with open(non_target_path, 'r') as file:\n",
    "            read_sequences = np.array([line.strip().upper() for line in file.readlines() if all(base in set(line) for base in 'ACTG')])\n",
    "            if non_target_sequences is not None:\n",
    "                non_target_sequences = np.concatenate([\n",
    "                        non_target_sequences,\n",
    "                        read_sequences\n",
    "                    ]) \n",
    "            else:\n",
    "                non_target_sequences = read_sequences\n",
    "                                    \n",
    "    return (target_sequences, non_target_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datasets(target_sequences, non_target_sequences):\n",
    "    \n",
    "    if target_sequences.shape[0] > non_target_sequences.shape[0]:\n",
    "        target_sequences_n = np.random.choice(\n",
    "            target_sequences,\n",
    "            non_target_sequences.shape[0],\n",
    "        )\n",
    "        non_target_sequences_n = non_target_sequences\n",
    "    else:\n",
    "        target_sequences_n = target_sequences\n",
    "        non_target_sequences_n = np.random.choice(\n",
    "            non_target_sequences,\n",
    "            target_sequences.shape[0],\n",
    "        )\n",
    "\n",
    "    X = np.concatenate((target_sequences_n, non_target_sequences_n))\n",
    "    Y = pd.Series(np.append(\n",
    "        np.full(target_sequences_n.shape[0], 1),\n",
    "        np.full(non_target_sequences_n.shape[0], 0))\n",
    "    )\n",
    "    \n",
    "    X = np.array([one_hot_dna(line) for line in X])\n",
    "    Y = keras.utils.to_categorical(Y)\n",
    "    \n",
    "    return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(X, Y, model):\n",
    "    # ROC vars\n",
    "    tprs = []\n",
    "    aucs, acc, rec, prec = [], [], [], []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    i = 0\n",
    "    # Precision-recall vars\n",
    "    precisions = []\n",
    "    best_precision = {\"precision_score\": 0.0, \"precision\": None, \"recall\": None}\n",
    "\n",
    "    folded_data = KFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "    for k, (train, test) in enumerate(folded_data.split(X, Y)):\n",
    "        Y_test_flat = np.array(list(map(lambda x: x[1] == 1 and 1 or 0, Y[test])))\n",
    "        model.fit(\n",
    "            X[train],\n",
    "            Y[train],\n",
    "            # Keras special args\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            validation_split=0.1,\n",
    "            callbacks=[EarlyStoppingCallback],\n",
    "            )\n",
    "        probas_ = model.predict(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test_flat, probas_[:, 1])\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        Y_pred = model.predict_classes(X[test])\n",
    "\n",
    "        acc.append(accuracy_score(Y_test_flat, Y_pred))\n",
    "        prec.append(precision_score(Y_test_flat, Y_pred))\n",
    "        rec.append(recall_score(Y_test_flat, Y_pred))\n",
    "        # Compute precision, recall\n",
    "        precision, recall, _ = precision_recall_curve(Y_test_flat, probas_[:, 1])\n",
    "        average_precision = average_precision_score(Y_test_flat, probas_[:, 1])\n",
    "        if average_precision > best_precision[\"precision_score\"]:\n",
    "            best_precision[\"precision\"] = precision\n",
    "            best_precision[\"recall\"] = recall\n",
    "            best_precision[\"precision_score\"] = average_precision\n",
    "        precisions.append(average_precision)\n",
    "        \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "\n",
    "    return mean_fpr, mean_tpr, best_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    session = keras.backend.get_session()\n",
    "    for layer in model.layers: \n",
    "#         if isinstance(layer, keras.engine.network.Network):\n",
    "#             reset_weights(layer)\n",
    "#             continue\n",
    "        for v in layer.__dict__.values():\n",
    "            if hasattr(v, 'initializer'):\n",
    "                v.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(datasets, model, model_name, write=True):\n",
    "    '''\n",
    "    datasets: a dict like {\n",
    "    'L1': 'data_link/L1/data/all_50_last.txt',\n",
    "    'p_pseudo': 'pseudogenes_50_last.txt',\n",
    "    }\n",
    "    model: keras model\n",
    "    model_name: str\n",
    "    '''\n",
    "    # Restart session to ensure that model is clean\n",
    "    reset_weights(model)\n",
    "\n",
    "    CLASS_1, CLASS_2 = datasets.keys()\n",
    "    target_pathes, non_target_pathes = datasets.values()\n",
    "    # Read data\n",
    "    target_sequences, non_target_sequences = read_dataframes(target_pathes, non_target_pathes)\n",
    "    X, Y = normalize_datasets(target_sequences, non_target_sequences)\n",
    "\n",
    "    # Use 5 fold to evaluate model\n",
    "    mean_fpr, mean_tpr, best_precision = compute_metrics(X, Y, model)\n",
    "\n",
    "    if write:\n",
    "        # Create file names\n",
    "        CSV_FILE_SUBNAME_OBJECTS = f\"{CLASS_1}_vs_{CLASS_2}\" # \"True_vs_False\"\n",
    "        CSV_FILE_SUBNAME = f\"{CSV_FILE_SUBNAME_OBJECTS}__{model_name}\"\n",
    "\n",
    "        # Write results to csv\n",
    "        # ROC\n",
    "        pd.DataFrame({\n",
    "            \"fpr\": mean_fpr,\n",
    "            \"tpr\": mean_tpr\n",
    "        }).to_csv(\n",
    "            f\"{CSV_RESULTS_FOLDER}/ROC__{CSV_FILE_SUBNAME}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "        # Precision-recall\n",
    "        pd.DataFrame({\n",
    "            \"precision\": best_precision[\"precision\"],\n",
    "            \"recall\": best_precision[\"recall\"]\n",
    "        }).to_csv(\n",
    "            f\"{CSV_RESULTS_FOLDER}/Precision-Recall__{CSV_FILE_SUBNAME}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "        #     pd.DataFrame(fi).to_csv(\"Feature_importance__{0}.csv\".format(CSV_FILE_SUBNAME), index=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'shuffled': ['data_link/L1/data/all_shuffled_50_last.txt', ],\n",
    "    },\n",
    "    {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'p_pseudo': ['pseudogenes_50_last.txt',],\n",
    "    },\n",
    "    {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'mRNA': ['KnownGene_50_last.txt',],\n",
    "    },\n",
    "    {\n",
    "    'L1_p_pseudo': ['data_link/L1/data/all_50_last.txt', 'pseudogenes_50_last.txt',],\n",
    "    'shuffled': ['data_link/L1/data/all_shuffled_50_last.txt', 'pseudogenes_50_last_shuffled.txt',],\n",
    "    },\n",
    "    {\n",
    "    'L1_mRNA': ['data_link/L1/data/all_50_last.txt', 'KnownGene_50_last.txt',],\n",
    "    'shuffled': ['data_link/L1/data/all_shuffled_50_last.txt', 'KnownGene_50_last_shuffled.txt',],\n",
    "    },\n",
    "    {\n",
    "    'mRNA': ['KnownGene_50_last.txt',],\n",
    "    'shuffled': ['KnownGene_50_last_shuffled.txt',],\n",
    "    },\n",
    "    {\n",
    "    'p_pseudo': ['pseudogenes_50_last.txt',],\n",
    "    'shuffled': ['pseudogenes_50_last_shuffled.txt',],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9535/9535 [==============================] - 0s 25us/sample - loss: 0.0823 - acc: 0.9723 - val_loss: 0.0371 - val_acc: 0.9925\n",
      "Train on 9536 samples, validate on 1060 samples\n",
      "Epoch 1/10\n",
      "9536/9536 [==============================] - 0s 24us/sample - loss: 0.0745 - acc: 0.9749 - val_loss: 0.0424 - val_acc: 0.9896\n",
      "Epoch 2/10\n",
      "9536/9536 [==============================] - 0s 24us/sample - loss: 0.0734 - acc: 0.9749 - val_loss: 0.0327 - val_acc: 0.9925\n",
      "Epoch 3/10\n",
      "9536/9536 [==============================] - 0s 24us/sample - loss: 0.0721 - acc: 0.9755 - val_loss: 0.0441 - val_acc: 0.9887\n",
      "Epoch 4/10\n",
      "9536/9536 [==============================] - 0s 24us/sample - loss: 0.0720 - acc: 0.9765 - val_loss: 0.0270 - val_acc: 0.9934\n",
      "Epoch 5/10\n",
      "9536/9536 [==============================] - 0s 25us/sample - loss: 0.0729 - acc: 0.9756 - val_loss: 0.0344 - val_acc: 0.9925\n",
      "Epoch 6/10\n",
      "9536/9536 [==============================] - 0s 23us/sample - loss: 0.0740 - acc: 0.9753 - val_loss: 0.0310 - val_acc: 0.9925\n",
      "Epoch 7/10\n",
      "9536/9536 [==============================] - 0s 24us/sample - loss: 0.0654 - acc: 0.9784 - val_loss: 0.0302 - val_acc: 0.9925\n",
      "Epoch 8/10\n",
      "9536/9536 [==============================] - 0s 25us/sample - loss: 0.0714 - acc: 0.9764 - val_loss: 0.0298 - val_acc: 0.9925\n",
      "Epoch 9/10\n",
      "9536/9536 [==============================] - 0s 24us/sample - loss: 0.0676 - acc: 0.9767 - val_loss: 0.0300 - val_acc: 0.9925\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    for experiment_dataset in experiments:\n",
    "        run_experiment(experiment_dataset, model, model_name)\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_vs_shuffled =  {\n",
    "    'L1': ['data_link/L1/data/all_50_last.txt',],\n",
    "    'shuffled': ['data_link/L1/data/all_shuffled_50_last.txt', ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    run_experiment(L1_vs_shuffled, model, model_name, write=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models and write results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_recognition_datasets = {\n",
    "    'mRNA': ['KnownGene_50_last.txt',],\n",
    "    'p_pseudo': ['pseudogenes_50_last.txt',],\n",
    "    'RP': ['RP_50_last.txt'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'Extra_recognition.csv', 'w') as file:\n",
    "    file.write(f'Model type,Train classes,Recognition class,% recognized\\n')\n",
    "for model_name, model in models.items():\n",
    "    for evaluated_on, target_path in extra_recognition_datasets.items():\n",
    "        dataset, _ = read_dataframes(target_path, [])\n",
    "        dataset = np.array([one_hot_dna(line) for line in dataset])\n",
    "        evaluation_result = np.mean(model.predict_classes(dataset))\n",
    "        with open(f'Extra_recognition.csv', 'a') as file:\n",
    "            file.write(f'{model_name},L1_vs_shuffled,{evaluated_on},{evaluation_result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
